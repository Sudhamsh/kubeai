{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"KubeAI: Private Open AI on Kubernetes","text":"<p>The simple AI platform that runs on Kubernetes.</p> <p>\"KubeAI is highly scalable, yet compact enough to fit on my old laptop.\" - Some Google Engineer</p> <p>\u2705\ufe0f  Drop-in replacement for OpenAI with API compatibility \u2696\ufe0f  Scale from zero, autoscale based on load \ud83d\ude80  Serve OSS LLMs on CPUs or GPUs \ud83d\udcac  Fast speed to text on CPU or GPU \ud83d\udee0\ufe0f  Zero dependencies (no Istio, Knative, etc.)  \ud83e\udd16  Operates OSS model servers (vLLM, Ollama, FasterWhisper) \ud83d\udd0b  Chat UI included (OpenWebUI \u2709\ufe0f  Batch inference with messaging systems (Kafka, PubSub, etc.)</p>"},{"location":"#architecture","title":"Architecture","text":"<p>KubeAI serves an OpenAI compatible HTTP API. Admins can configure ML models via <code>kind: Model</code> Kubernetes Custom Resources. KubeAI can be thought of as a Model Operator (See Operator Pattern) that manages vLLM and Ollama servers.</p> <p></p>"},{"location":"#local-quickstart","title":"Local Quickstart","text":"<p>Create a local cluster using kind or minikube.</p> TIP: If you are using Podman for kind... Make sure your Podman machine can use up to 6G of memory (by default it is capped at 2G):  <pre><code># You might need to stop and remove the existing machine:\npodman machine stop\npodman machine rm\n\n# Init and start a new machine:\npodman machine init --memory 6144\npodman machine start\n</code></pre> <pre><code>kind create cluster # OR: minikube start\n</code></pre> <p>Add the KubeAI Helm repository.</p> <pre><code>helm repo add kubeai https://substratusai.github.io/kubeai/\nhelm repo update\n</code></pre> <p>Install KubeAI and wait for all components to be ready (may take a minute).</p> <pre><code>cat &lt;&lt;EOF &gt; helm-values.yaml\nmodels:\n  catalog:\n    gemma2-2b-cpu:\n      enabled: true\n      minReplicas: 1\n    qwen2-500m-cpu:\n      enabled: true\n    nomic-embed-text-cpu:\n      enabled: true\nEOF\n\nhelm upgrade --install kubeai kubeai/kubeai \\\n    -f ./helm-values.yaml \\\n    --wait --timeout 10m\n</code></pre> <p>Before progressing to the next steps, start a watch on Pods in a standalone terminal to see how KubeAI deploys models. </p> <pre><code>kubectl get pods --watch\n</code></pre>"},{"location":"#interact-with-gemma2","title":"Interact with Gemma2","text":"<p>Because we set <code>minReplicas: 1</code> for the Gemma model you should see a model Pod already coming up.</p> <p>Start a local port-forward to the bundled chat UI.</p> <pre><code>kubectl port-forward svc/openwebui 8000:80\n</code></pre> <p>Now open your browser to localhost:8000 and select the Gemma model to start chatting with.</p>"},{"location":"#scale-up-qwen2-from-zero","title":"Scale up Qwen2 from Zero","text":"<p>If you go back to the browser and start a chat with Qwen2, you will notice that it will take a while to respond at first. This is because we set <code>minReplicas: 0</code> for this model and KubeAI needs to spin up a new Pod (you can verify with <code>kubectl get models -oyaml qwen2-500m-cpu</code>).</p> <p>NOTE: Autoscaling after initial scale-from-zero is not yet supported for the Ollama backend which we use in this local quickstart. KubeAI relies upon backend-specific metrics and the Ollama project has an open issue: https://github.com/ollama/ollama/issues/3144. To see autoscaling in action, checkout the GKE install guide which uses the vLLM backend and autoscales across GPU resources.</p>"},{"location":"#supported-models","title":"Supported Models","text":"<p>Any vLLM or Ollama model can be served by KubeAI. Some examples of popular models served on KubeAI include:</p> <ul> <li>Llama v3.1 (8B, 70B, 405B) </li> <li>Gemma2 (2B, 9B, 27B)</li> <li>Qwen2 (1.5B, 7B, 72B)</li> </ul>"},{"location":"#documentation","title":"Documentation","text":"<p>Checkout our documenation on kubeai.org to find info on:</p> <ul> <li>Installing KubeAI in the cloud</li> <li>How to guides (e.g. how to manage models and resource profiles).</li> <li>Concepts (how the components of KubeAI work).</li> <li>How to contribute</li> </ul>"},{"location":"#openai-api-compatibility","title":"OpenAI API Compatibility","text":"<pre><code># Implemented #\n/v1/chat/completions\n/v1/completions\n/v1/embeddings\n/v1/models\n/v1/audio/transcriptions\n\n# Planned #\n# /v1/assistants/*\n# /v1/batches/*\n# /v1/fine_tuning/*\n# /v1/images/*\n# /v1/vector_stores/*\n</code></pre>"},{"location":"#immediate-roadmap","title":"Immediate Roadmap","text":"<ul> <li>Model caching</li> <li>LoRA finetuning (compatible with OpenAI finetuning API)</li> <li>Image generation (compatible with OpenAI images API)</li> </ul> <p>NOTE: KubeAI was born out of a project called Lingo which was a simple Kubernetes LLM proxy with basic autoscaling. We relaunched the project as KubeAI (late August 2024) and expanded the roadmap to what it is today.</p> <p>\ud83c\udf1f Don't forget to drop us a star on GitHub and follow the repo to stay up to date!</p> <p></p>"},{"location":"#contact","title":"Contact","text":"<p>Let us know about features you are interested in seeing or reach out with questions. Visit our Discord channel to join the discussion!</p> <p>Or just reach out on LinkedIn if you want to connect:</p> <ul> <li>Nick Stogner</li> <li>Sam Stoelinga</li> </ul>"},{"location":"concepts/models/","title":"Models","text":"<p>KubeAI serves ML models by launching Pods on Kubernetes. Every model server Pod loads exactly one model on startup.</p> <p>Model Custom Resources are used to configure what ML models KubeAI serves.</p> <p>Example:</p> <pre><code>apiVersion: kubeai.org/v1\nkind: Model\nmetadata:\n  name: llama-3.1-8b-instruct-fp8-l4\nspec:\n  features: [\"TextGeneration\"]\n  owner: neuralmagic\n  url: hf://neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8\n  engine: VLLM\n  args:\n    - --max-model-len=16384\n    - --max-num-batched-token=16384\n    - --gpu-memory-utilization=0.9\n  minReplicas: 0\n  maxReplicas: 3\n  resourceProfile: nvidia-gpu-l4:1\n</code></pre>"},{"location":"concepts/models/#server-settings","title":"Server Settings","text":"<p>In a Model manifest you can define what server to use for inference (<code>VLLM</code>, <code>OLlama</code>). Any model-specific settings can be passed to the server process via the <code>args</code> and <code>env</code> fields.</p>"},{"location":"concepts/models/#autoscaling","title":"Autoscaling","text":"<p>KubeAI has built-in support for autoscaling model inference servers. If the model is scaled to zero when a request comes in, the KubeAI server will hold the request until it is able spin up a new server Pod. Once the model server is running, KubeAI server will automatically scrape its metrics endpoint to determine how to autoscale from there.</p> <p>Autoscaling can be configured via the <code>minReplicas</code> and <code>maxReplicas</code> spec fields. KubeAI will modify the <code>.spec.replicas</code> field as it autoscales and report the observed state in <code>.status.replicas</code>.</p>"},{"location":"concepts/models/#openai-api-compatibility","title":"OpenAI-API compatibility","text":"<p>Kubernetes Model Custom Resources are the source of truth for models that exist in KubeAI. KubeAI provides a view into installed Models via the OpenAI <code>/v1/models</code> endpoint (which KubeAI serves at <code>/openai/v1/models</code>).</p>"},{"location":"concepts/models/#next","title":"Next","text":"<p>Read about how to manage models.</p>"},{"location":"concepts/resource-profiles/","title":"Resource Profiles","text":"<p>A resource profile maps a type of compute resource (i.e. NVIDIA L4 GPU) to a collection of Kubernetes settings that are configured on inference server Pods. These profiles are defined in the KubeAI <code>config.yaml</code> file (via a ConfigMap). Each model specifies the resource profile that it requires.</p> <p>Kubernetes Model resources specify a resource profile and the count of that resource that they require:</p> <pre><code># model.yaml\napiVersion: kubeai.org/v1\nkind: Model\nmetadata:\n  name: llama-3.1-8b-instruct-fp8-l4\nspec:\n  engine: VLLM\n  resourceProfile: nvidia-gpu-l4:1 # Specified as &lt;profile&gt;:&lt;count&gt;\n  # ...\n</code></pre> <p>A given profile might need to contain slightly different settings based on the cluster/cloud that KubeAI is deployed in.</p> <p>Example: A resource profile named <code>nvidia-gpu-l4</code> might contain the following settings on a GKE Kubernetes cluster:</p> <pre><code># KubeAI config.yaml\nresourceProfiles:\n  nvidia-gpu-l4:\n    limits:\n      # Typical across most Kubernetes clusters:\n      nvidia.com/gpu: \"1\"\n    requests:\n      nvidia.com/gpu: \"1\"\n    nodeSelector:\n      # Specific to GKE:\n      cloud.google.com/gke-accelerator: \"nvidia-l4\"\n      cloud.google.com/gke-spot: \"true\"\n    imageName: \"nvidia-gpu\"\n</code></pre> <p>In addition to node selectors and resource requirements, a resource profile may optionally specify an image name. This name maps to the container image that will be selected when serving a model on that resource:</p> <pre><code># KubeAI config.yaml\nmodelServers:\n  VLLM:\n    images:\n      default: \"vllm/vllm-openai:v0.5.5\"\n      nvidia-gpu: \"vllm/vllm-openai:v0.5.5\" # &lt;--\n      cpu: \"vllm/vllm-openai-cpu:v0.5.5\"\n  OLlama:\n    images:\n      # ...\n</code></pre>"},{"location":"concepts/resource-profiles/#next","title":"Next","text":"<p>Read about how to manage resource profiles.</p>"},{"location":"contributing/development-environment/","title":"Development environment","text":"<p>This document provides instructions for setting up an environment for developing KubeAI.</p>"},{"location":"contributing/development-environment/#optional-cloud-setup","title":"Optional: Cloud Setup","text":""},{"location":"contributing/development-environment/#gcp-pubsub","title":"GCP PubSub","text":"<p>If you are develop PubSub messaging integration on GCP, setup test topics and subscriptions and uncomment the <code>.messaging.streams</code> in <code>./hack/dev-config.yaml</code>.</p> <pre><code>gcloud auth login --update-adc\n\ngcloud pubsub topics create test-kubeai-requests\ngcloud pubsub subscriptions create test-kubeai-requests-sub --topic test-kubeai-requests\ngcloud pubsub topics create test-kubeai-responses\ngcloud pubsub subscriptions create test-kubeai-responses-sub --topic test-kubeai-responses\n</code></pre>"},{"location":"contributing/development-environment/#run-in-local-cluster","title":"Run in Local Cluster","text":"<pre><code>kind create cluster\n# OR\n#./hack/create-dev-gke-cluster.yaml\n\n# Generate CRDs from Go code.\nmake generate &amp;&amp; make manifests\n\n# When CRDs are changed reapply using kubectl:\nkubectl apply -f ./charts/kubeai/charts/crds/crds\n\n# Model with special address annotations:\nkubectl apply -f ./hack/dev-model.yaml\n\n# OPTION A #\n# Run KubeAI inside cluster\n# Change `-f` based on the cluster environment.\nhelm upgrade --install kubeai ./charts/kubeai \\\n    --set openwebui.enabled=true \\\n    --set image.tag=latest \\\n    --set image.pullPolicy=Always \\\n    --set image.repository=us-central1-docker.pkg.dev/substratus-dev/default/kubeai \\\n    --set secrets.huggingface.token=$HUGGING_FACE_HUB_TOKEN \\\n    --set replicaCount=1 -f ./hack/dev-gke-helm-values.yaml\n\n# OPTION B #\n# For quick local interation (run KubeAI outside of cluster)\nCONFIG_PATH=./hack/dev-config.yaml POD_NAMESPACE=default go run ./cmd/main.go --allow-pod-address-override\n\n# In another terminal:\nwhile true; do kubectl port-forward service/dev-model 7000:7000; done\n############\n</code></pre>"},{"location":"contributing/development-environment/#running","title":"Running","text":""},{"location":"contributing/development-environment/#completions-api","title":"Completions API","text":"<pre><code># If you are running kubeai in-cluster:\n# kubectl port-forward svc/kubeai 8000:80\n\ncurl http://localhost:8000/openai/v1/completions -H \"Content-Type: application/json\" -d '{\"prompt\": \"Hi\", \"model\": \"dev\"}' -v\n</code></pre>"},{"location":"contributing/development-environment/#messaging-integration","title":"Messaging Integration","text":"<pre><code>gcloud pubsub topics publish test-kubeai-requests \\                  \n  --message='{\"path\":\"/v1/completions\", \"metadata\":{\"a\":\"b\"}, \"body\": {\"model\": \"dev\", \"prompt\": \"hi\"}}'\n\ngcloud pubsub subscriptions pull test-kubeai-responses-sub --auto-ack\n</code></pre>"},{"location":"contributing/documentation/","title":"Documentation","text":"<p>We are grateful for anyone who takes the time to improve KubeAI documentation! In order to keep our docs clear and consistent we ask that you first read about the approach to documentation that we have standardized on...</p>"},{"location":"contributing/documentation/#read-before-writing","title":"Read before writing!","text":"<p>The KubeAI approach to documentation is loosely inspired by the Diataxis method.</p> <p>TLDR on how KubeAI docs are organized:</p> <ul> <li>Installation: How-to guides specific to installing KubeAI.</li> <li>How To: Directions that guide the reader through a problem or towards a result. How-to guides are goal-oriented. They assume the user is familiar with general concepts, tools, and has already installed KubeAI.</li> <li>Concepts: A reflective explanation of KubeAI topics with a focus on giving the reader an understanding of the why.</li> <li>Tutorials: Learning oriented experiences. Lessons that often guide a user from beginning to end. The goal is to help the reader learn something (compared to a how-to guide that is focused on helping the reader do something).</li> <li>Contributing: The docs in here differ from the rest of the docs by audience: these docs are for anyone who will be contributing code or docs to the KubeAI project.</li> </ul>"},{"location":"contributing/documentation/#how-to-serve-kubeaiorg-locally","title":"How to serve kubeai.org locally","text":"<p>Make sure you have python3 installed and run:</p> <pre><code>make docs\n</code></pre>"},{"location":"how-to/configure-speech-to-text/","title":"Configure Speech To Text","text":"<p>KubeAI provides a Speech to Text endpoint that can be used to transcribe audio files. This guide will walk you through the steps to enable this feature.</p>"},{"location":"how-to/configure-speech-to-text/#enable-speech-to-text-model","title":"Enable Speech to Text model","text":"<p>You can create neew models by creating a Model CRD object or by enabling a model from the model catalog.</p>"},{"location":"how-to/configure-speech-to-text/#enable-from-model-catalog","title":"Enable from model catalog","text":"<p>KubeAI provides predefined models in the model catalog. To enable the Speech to Text model, you can set the <code>enabled</code> flag to <code>true</code> in the <code>helm-values.yaml</code> file.</p> <pre><code>models:\n  catalog:\n    faster-whisper-medium-en-cpu:\n      enabled: true\n      minReplicas: 1\n</code></pre>"},{"location":"how-to/configure-speech-to-text/#enable-by-creating-model-crd","title":"Enable by creating Model CRD","text":"<p>You can also create a Model CRD object to enable the Speech to Text model. Here is an example of a Model CRD object for the Speech to Text model:</p> <pre><code>apiVersion: kubeai.org/v1\nkind: Model\nmetadata:\n  name: faster-whisper-medium-en-cpu\nspec:\n  features: [SpeechToText]\n  owner: Systran\n  url: hf://Systran/faster-whisper-medium.en\n  engine: FasterWhisper\n  minReplicas: 0\n  maxReplicas: 3\n  resourceProfile: cpu:1\n</code></pre>"},{"location":"how-to/configure-speech-to-text/#usage","title":"Usage","text":"<p>The Speech to Text endpoint is available at <code>/openai/v1/transcriptions</code>.</p> <p>Example usage using curl:</p> <pre><code>curl -L -o kubeai.mp4 https://github.com/user-attachments/assets/711d1279-6af9-4c6c-a052-e59e7730b757\ncurl http://localhost:8000/openai/v1/audio/transcriptions \\\n  -F \"file=@kubeai.mp4\" \\\n  -F \"language=en\" \\\n  -F \"model=faster-whisper-medium-en-cpu\"\n</code></pre>"},{"location":"how-to/manage-models/","title":"Manage models","text":"<p>This guide provides instructions on how to perform CRUD operations on KubeAI Models.</p>"},{"location":"how-to/manage-models/#listing-models","title":"Listing models","text":"<p>You can view all installed models through the Kubernetes API using <code>kubectl get models</code> (use the <code>-o yaml</code> flag for more details).</p> <p>You can also list all models via the OpenAI-compatible <code>/v1/models</code> endpoint:</p> <pre><code>curl http://your-deployed-kubeai-endpoint/openai/v1/models\n</code></pre>"},{"location":"how-to/manage-models/#installing-models-with-helm","title":"Installing models with helm","text":""},{"location":"how-to/manage-models/#preconfigured-models-with-helm","title":"Preconfigured models with helm","text":"<p>When you are defining KubeAI Helm values, you can install a preconfigured Model by setting <code>enabled: true</code>. You can view a list of all preconfigured models here. NOTE: When you are installing the KubeAI chart, the catalog is accessed under <code>.models.catalog.&lt;model-name&gt;</code>:</p> <pre><code># helm-values.yaml\nmodels:\n  catalog:\n    llama-3.1-8b-instruct-fp8-l4:\n      enabled: true\n</code></pre> <p>You can optionally override preconfigured settings, for example, <code>resourceProfile</code>:</p> <pre><code># helm-values.yaml\nmodels:\n  catalog:\n    llama-3.1-8b-instruct-fp8-l4:\n      enabled: true\n      resourceProfile: nvidia-gpu-l4:2 # Require \"2 NVIDIA L4 GPUs\"\n</code></pre>"},{"location":"how-to/manage-models/#custom-models-with-helm","title":"Custom models with helm","text":"<p>If you prefer to add a custom model via the same Helm chart you use for installed KubeAI, you can add your custom model entry into the <code>.models.catalog</code> array of your existing Helm values file:</p> <pre><code># helm-values.yaml\nmodels:\n  catalog:\n    my-custom-model-name:\n      enabled: true\n      features: [\"TextEmbedding\"]\n      owner: me\n      url: \"hf://me/my-custom-model\"\n      resourceProfile: CPU:1\n</code></pre>"},{"location":"how-to/manage-models/#installing-models-with-kubectl","title":"Installing models with kubectl","text":"<p>You can add your own model by defining a Model yaml file and applying it using <code>kubectl apply -f model.yaml</code>.</p> <p>If you have a running cluster with KubeAI installed you can inspect the schema for a Model using <code>kubectl explain</code>:</p> <pre><code>kubectl explain models\nkubectl explain models.spec\nkubectl explain models.spec.engine\n</code></pre>"},{"location":"how-to/manage-models/#feedback-welcome-a-model-management-ui","title":"Feedback welcome: A model management UI","text":"<p>We are considering adding a UI for managing models in a running KubeAI instance. Give the GitHub Issue a thumbs up if you would be interested in this feature.</p>"},{"location":"how-to/manage-resource-profiles/","title":"Manage resource profiles","text":"<p>This guide will cover modifying preconfigured resource profiles and adding your own.</p>"},{"location":"how-to/manage-resource-profiles/#modifying-preconfigured-resource-profiles","title":"Modifying preconfigured resource profiles","text":"<p>The KubeAI helm chart comes with preconfigured resource profiles for common resource types such as NVIDIA L4 GPUs. You can view these profiles in the default helm values file.</p> <p>These profiles usually require some additional settings based on the cluster/cloud that KubeAI is installed into. You can modify a resource profile by setting custom helm values and runing <code>helm install</code> or <code>helm upgrade</code>. For example, if you are installing KubeAI on GKE you will need to set GKE-specific node selectors:</p> <pre><code># helm-values.yaml\nresourceProfiles:\n  nvidia-gpu-l4:\n    nodeSelector:\n      cloud.google.com/gke-accelerator: \"nvidia-l4\"\n      cloud.google.com/gke-spot: \"true\"\n</code></pre> <p>NOTE: See the cloud-specific installation guide for a comprehensive list of settings.</p>"},{"location":"how-to/manage-resource-profiles/#adding-additional-resource-profiles","title":"Adding additional resource profiles","text":"<p>If the preconfigured resource profiles do not meet your needs you can add additional profiles by appending to the <code>.resourceProfiles</code> object in the helm values file you use to install KubeAI.</p> <pre><code># helm-values.yaml\nresourceProfiles:\n  my-custom-gpu:\n    imageName: \"optional-custom-image-name\"\n    nodeSelector:\n      my-custom-node-pool: \"some-value\"\n    limits:\n      custom.com/gpu: \"1\"\n    requests:\n      custom.com/gpu: \"1\"\n      cpu: \"3\"\n      memory: \"12Gi\"\n</code></pre> <p>If you need to run custom model server images on your resource profile, make sure to also add those in the <code>modelServers</code> section:</p> <pre><code># helm-values.yaml\nmodelServers:\n  VLLM:\n    images:\n      optional-custom-image-name: \"my-repo/my-vllm-image:v1.2.3\"\n  OLlama:\n    images:\n      optional-custom-image-name: \"my-repo/my-ollama-image:v1.2.3\"\n</code></pre>"},{"location":"how-to/manage-resource-profiles/#next","title":"Next","text":"<p>See the guide on how to manage models which includes how to configure the resource profile to use for a given model.</p>"},{"location":"installation/gke/","title":"Install on GKE","text":"TIP: Make sure you have enough quota in your GCP project. <p>Open the cloud console quotas page: https://console.cloud.google.com/iam-admin/quotas. Make sure your project is selected in the top left.</p> <p>There are 3 critical quotas you will need to verify for this guide. The minimum value here is assuming that you have nothing else running in your project.</p> Quota Location Min Value Preemptible NVIDIA L4 GPUs <code>&lt;your-region&gt;</code> 2 GPUs (all regions) - 2 CPUs (all regions) - 24 <p>See the following screenshot examples of how these quotas appear in the console:</p> <p></p> <p></p> <p></p>"},{"location":"installation/gke/#1-create-a-cluster","title":"1. Create a cluster","text":""},{"location":"installation/gke/#option-gke-autopilot","title":"Option: GKE Autopilot","text":"<p>Create an Autopilot cluster (replace <code>us-central1</code> with a region that you have quota).</p> <pre><code>gcloud container clusters create-auto cluster-1 \\\n    --location=us-central1\n</code></pre>"},{"location":"installation/gke/#option-gke-standard","title":"Option: GKE Standard","text":"<p>TODO: Reference gcloud commands for creating a GKE standard cluster.</p>"},{"location":"installation/gke/#2-install-kubeai","title":"2. Install KubeAI","text":"<p>Define the installation values for GKE.</p> <pre><code>cat &lt;&lt;EOF &gt; helm-values.yaml\nmodels:\n  catalog:\n    llama-3.1-8b-instruct-fp8-l4:\n      enabled: true\n\nresourceProfiles:\n  nvidia-gpu-l4:\n    nodeSelector:\n      cloud.google.com/gke-accelerator: \"nvidia-l4\"\n      cloud.google.com/gke-spot: \"true\"\nEOF\n</code></pre> <p>Make sure you have a HuggingFace Hub token set in your environment (<code>HUGGING_FACE_HUB_TOKEN</code>).</p> <p>Install KubeAI with Helm.</p> <pre><code>helm repo add kubeai https://substratusai.github.io/kubeai/\nhelm repo update\n\nhelm upgrade --install kubeai kubeai/kubeai \\\n    -f ./helm-values.yaml \\\n    --set secrets.huggingface.token=$HUGGING_FACE_HUB_TOKEN \\\n    --wait\n</code></pre>"},{"location":"tutorials/langtrace/","title":"Deploying KubeAI with Langtrace","text":"<p>Langtrace is an open source tool that helps you with tracing and monitoring your AI calls. It includes a self-hosted UI that for example shows you the estimated costs of your LLM calls.</p> <p>KubeAI is used for deploying LLMs with an OpenAI compatible endpoint.</p> <p>In this tutorial you will learn how to deploy KubeAI and Langtrace end-to-end. Both KubeAI and Langtrace are installed in your Kubernetes cluster. No cloud services or external dependencies are required.</p> <p>If you don't have a K8s cluster yet, you can create one using kind or minikube. <pre><code>kind create cluster # OR: minikube start\n</code></pre></p> <p>Install Langtrace: <pre><code>helm repo add langtrace https://Scale3-Labs.github.io/langtrace-helm-chart\nhelm repo update\nhelm install langtrace langtrace/langtrace\n</code></pre></p> <p>Install KubeAI: <pre><code>helm repo add kubeai https://substratusai.github.io/kubeai/\nhelm repo update\ncat &lt;&lt;EOF &gt; helm-values.yaml\nmodels:\n  catalog:\n    gemma2-2b-cpu:\n      enabled: true\n      minReplicas: 1\nEOF\n\nhelm upgrade --install kubeai kubeai/kubeai \\\n    --wait --timeout 10m \\\n    -f ./helm-values.yaml\n</code></pre></p> <p>Create a local Python environment and install dependencies: <pre><code>python3 -m venv .venv\nsource .venv/bin/activate\npip install langtrace-python-sdk openai\n</code></pre></p> <p>Expose the KubeAI service to your local port: <pre><code>kubectl port-forward service/kubeai 8000:80\n</code></pre></p> <p>Expose the Langtrace service to your local port: <pre><code>kubectl port-forward service/langtrace-app 3000:3000\n</code></pre></p> <p>A Langtrace API key is required to use the Langtrace SDK. So lets get one by visiting your self hosted Langtace UI.</p> <p>Open your browser to http://localhost:3000, create a project and get the API keys for your langtrace project.</p> <p>In the Python script below, replace <code>langtrace_api_key</code> with your API key.</p> <p>Create file named <code>langtrace-example.py</code> with the following content: <pre><code># Replace this with your langtrace API key by visiting http://localhost:3000\nlangtrace_api_key=\"f7e003de19b9a628258531c17c264002e985604ca9fa561debcc85c41f357b09\"\n\nfrom langtrace_python_sdk import langtrace\nfrom langtrace_python_sdk.utils.with_root_span import with_langtrace_root_span\n# Paste this code after your langtrace init function\n\nfrom openai import OpenAI\n\nlangtrace.init(\n    api_key=api_key,\n    api_host=\"http://localhost:3000/api/trace\",\n)\n\nbase_url = \"http://localhost:8000/openai/v1\"\nmodel = \"gemma2-2b-cpu\"\n\n@with_langtrace_root_span()\ndef example():\n    client = OpenAI(base_url=base_url, api_key=\"ignored-by-kubeai\")\n    response = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"How many states of matter are there?\"\n            }\n        ],\n    )\n    print(response.choices[0].message.content)\n\nexample()\n</code></pre></p> <p>Run the Python script: <pre><code>python3 langtrace-example.py\n</code></pre></p> <p>Now you should see the trace in your Langtrace UI. Take a look by visiting http://localhost:3000.</p> <p></p>"}]}